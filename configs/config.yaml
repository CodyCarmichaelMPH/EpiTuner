# EpiTuner Configuration
# Single config that adapts to any hardware

# Model settings
model:
  max_length: 512
  trust_remote_code: false

# Training settings  
training:
  epochs: 2
  learning_rate: 2e-4
  batch_size: 1  # Conservative default
  gradient_accumulation_steps: 4
  warmup_steps: 10
  
# LoRA settings
lora:
  r: 8  # Conservative rank
  alpha: 16
  dropout: 0.1
  target_modules: ["q_proj", "v_proj"]  # Safe modules

# Data processing
data:
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  
# Output settings
output:
  save_steps: 50
  logging_steps: 10
  save_total_limit: 1

